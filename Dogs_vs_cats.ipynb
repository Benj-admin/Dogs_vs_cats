{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rETqrWXSlXIW",
        "Td4zPBwsTeFX"
      ],
      "authorship_tag": "ABX9TyNv0eBqcWubQNm4N8FF4Rvh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benj-admin/Dogs_vs_cats/blob/main/Dogs_vs_cats.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  CNN Project: Cats vs. Dogs Classification with PyTorch 🐈🐕\n",
        "\n",
        "This project aims to build a Convolutional Neural Network (CNN) using the **PyTorch** framework to accurately distinguish between images of cats and dogs.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "HhK6gv3ZQAD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Kaggle Connection and Data Acquisition\n",
        "\n",
        "This initial step configures your Google Colab environment to interact with the Kaggle API, allowing you to download the large **\"Dogs vs. Cats\"** dataset directly. We'll install the necessary library, securely upload your API token, and download/unzip the raw image data."
      ],
      "metadata": {
        "id": "rETqrWXSlXIW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPoUybkOMHKS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb00b9b0-c864-431c-ce89-df78f974e7bc"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Kaggle API setup complete. Starting download...\n",
            "dogs-vs-cats.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace sampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: replace train/cat.0.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "# Install the Kaggle API tool quietly\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Create the hidden directory for Kaggle configuration\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# MANUAL STEP: Upload your 'kaggle.json' file to the Colab file system (left panel).\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# Securely copy the API token and set restrictive permissions\n",
        "# 'kaggle.json' must be in the Colab root directory\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "print(\"Kaggle API setup complete. Starting download...\")\n",
        "\n",
        "# Download the competition dataset (contains train.zip and test1.zip)\n",
        "!kaggle competitions download -c dogs-vs-cats\n",
        "\n",
        "# Unzip the main download file\n",
        "!unzip -q dogs-vs-cats.zip\n",
        "\n",
        "# Unzip the training data file (25,000 labellées images)\n",
        "!unzip -q train.zip\n",
        "print(\"Training data downloaded and extracted into the 'train/' folder.\")\n",
        "\n",
        "# Verify the file structure\n",
        "!ls train | head -n 5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Data Organization and Preprocessing\n",
        "\n",
        "The raw images are currently mixed in the `train/` folder. PyTorch's built-in `ImageFolder` class requires data to be organized into class-specific subfolders (here, `train_set/cats` and `train_set/dogs`).\n",
        "\n",
        "In this step, we will:\n",
        "1.  Create the necessary directory structure (`train_set` and `val_set`).\n",
        "2.  Split the data (e.g., 80% for training, 20% for validation) and move the images accordingly.\n",
        "3.  Define **data augmentation** techniques for the training set and standard transformations for the validation set using `torchvision.transforms`.\n",
        "4.  Create the PyTorch **`DataLoader`** instances for batch-wise data access."
      ],
      "metadata": {
        "id": "Td4zPBwsTeFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch"
      ],
      "metadata": {
        "id": "hKQWjT0DVrSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Define Paths and Parameters ---\n",
        "\n",
        "# Source directory containing all images\n",
        "SOURCE_DIR = 'train'\n",
        "# Base directory for the organized data\n",
        "BASE_DIR = 'data'\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, 'train_set')\n",
        "VAL_DIR = os.path.join(BASE_DIR, 'val_set')\n",
        "\n",
        "# Hyperparameters for preprocessing\n",
        "IMAGE_SIZE = 224 # Standard input size\n",
        "BATCH_SIZE = 32\n",
        "SPLIT_RATIO = 0.8 # 80% for training, 20% for validation\n",
        "\n",
        "# Standard Normalization values derived from ImageNet\n",
        "MEAN = [0.485, 0.456, 0.406]\n",
        "STD = [0.229, 0.224, 0.225]"
      ],
      "metadata": {
        "id": "6NPguORGV5gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 2. Create Directory Structure and Split Data ---\n",
        "\n",
        "# Create the main directories\n",
        "os.makedirs(os.path.join(TRAIN_DIR, 'cats'), exist_ok=True)\n",
        "os.makedirs(os.path.join(TRAIN_DIR, 'dogs'), exist_ok=True)\n",
        "os.makedirs(os.path.join(VAL_DIR, 'cats'), exist_ok=True)\n",
        "os.makedirs(os.path.join(VAL_DIR, 'dogs'), exist_ok=True)\n",
        "\n",
        "# List and shuffle all files by class\n",
        "all_files = os.listdir(SOURCE_DIR)\n",
        "cat_files = [f for f in all_files if f.startswith('cat')]\n",
        "dog_files = [f for f in all_files if f.startswith('dog')]\n",
        "random.shuffle(cat_files)\n",
        "random.shuffle(dog_files)\n",
        "\n",
        "# Split and copy utility function\n",
        "def split_and_copy(files, class_name, split_ratio):\n",
        "    train_split = int(len(files) * split_ratio)\n",
        "    train_files = files[:train_split]\n",
        "    val_files = files[train_split:]\n",
        "\n",
        "    for f in train_files:\n",
        "        shutil.copy(os.path.join(SOURCE_DIR, f), os.path.join(TRAIN_DIR, class_name, f))\n",
        "    for f in val_files:\n",
        "        shutil.copy(os.path.join(SOURCE_DIR, f), os.path.join(VAL_DIR, class_name, f))\n",
        "\n",
        "# Apply the split and copy function\n",
        "split_and_copy(cat_files, 'cats', SPLIT_RATIO)\n",
        "split_and_copy(dog_files, 'dogs', SPLIT_RATIO)\n",
        "\n"
      ],
      "metadata": {
        "id": "fzzy3kpUOgK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Data split into Train and Validation sets.\")\n",
        "print(f\"Total training images: {len(os.listdir(os.path.join(TRAIN_DIR, 'cats'))) + len(os.listdir(os.path.join(TRAIN_DIR, 'dogs')))}\")\n",
        "print(f\"Total validation images: {len(os.listdir(os.path.join(VAL_DIR, 'cats'))) + len(os.listdir(os.path.join(VAL_DIR, 'dogs')))}\")"
      ],
      "metadata": {
        "id": "w02h_rLUXvZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Define Transformations and DataLoaders ---\n",
        "\n",
        "# Transformations for training (including Data Augmentation)\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.RandomRotation(15),             # Simple data augmentation, helping the model become invariant to the rotation\n",
        "    transforms.RandomHorizontalFlip(),         # Simple data augmentation, helping the model become invariant to the symmetry\n",
        "    transforms.ToTensor(),                     # Convert image to a PyTorch Tensor\n",
        "    transforms.Normalize(mean=MEAN, std=STD)   # Normalize pixel values\n",
        "])\n",
        "\n",
        "# Transformations for validation (only resizing, ToTensor, and Normalization)\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=MEAN, std=STD)\n",
        "])\n",
        "\n",
        "# Use ImageFolder to load the dataset structure\n",
        "train_data = datasets.ImageFolder(root=TRAIN_DIR, transform=train_transforms)\n",
        "val_data = datasets.ImageFolder(root=VAL_DIR, transform=val_transforms)\n",
        "\n",
        "# Create DataLoaders to iterate over batches\n",
        "# num_workers > 0 speeds up loading by using multiple subprocesses\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"\\nPyTorch DataLoaders created successfully.\")\n",
        "print(f\"Classes: {train_data.classes}\")\n",
        "print(f\"Number of training batches (steps per epoch): {len(train_loader)}\")"
      ],
      "metadata": {
        "id": "ReIB2U6BV_4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 3: Building and Training the Baseline Model\n",
        "\n",
        "In this phase, we design a simple Convolutional Neural Network (CNN) from scratch using PyTorch's `torch.nn.Module`. This initial model, often called a **baseline**, will serve as a starting point to assess the difficulty of the task and establish a benchmark performance before implementing more advanced techniques.\n",
        "\n",
        "We will:\n",
        "1.  Define the CNN architecture using convolutional layers, ReLU activation, and max-pooling.\n",
        "2.  Choose the **Loss Function** (`CrossEntropyLoss`) and the **Optimizer** (`Adam`).\n",
        "3.  Implement the training loop over a few epochs."
      ],
      "metadata": {
        "id": "4tMNoXngbWld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import time"
      ],
      "metadata": {
        "id": "1qlw78aablsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Model Definition: Simple CNN Architecture ---\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # First Convolutional Block\n",
        "        # Input: 3x224x224 (RGB image)\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
        "\n",
        "        # Second Convolutional Block\n",
        "        # Input: 32x112x112 (after first pooling)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "        # Third Convolutional Block\n",
        "        # Input: 64x56x56 (after second pooling)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "\n",
        "        #  Pooling Layer\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        # Flatten Layer\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # Linear layers for classification\n",
        "        # Input: 128 * 28 * 28 (after third pooling and flatten layer)\n",
        "        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
        "        # Output layer: 2 classes (cat or dog)\n",
        "        self.fc2 = nn.Linear(512, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 3 block: Conv -> ReLU -> Pool\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x))) # Size: 32x112x112\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x))) # Size: 64x56x56\n",
        "        x = self.pool(nn.functional.relu(self.conv3(x))) # Size: 128x28x28\n",
        "\n",
        "        # Flatten the feature maps for the dense layers\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # Dense layers\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x) # Output logits for 2 classes\n",
        "        return x"
      ],
      "metadata": {
        "id": "2_sQZJ5nbswm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Setup Device, Model, Loss, and Optimizer ---\n",
        "\n",
        "# Check for GPU availability and select device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Instantiate the model and move it to the selected device\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "# Define Loss Function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Number of epochs to train for the baseline\n",
        "NUM_EPOCHS = 5\n"
      ],
      "metadata": {
        "id": "Zc0z3EyugJuI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7451a83-7a8d-43eb-eaed-f86c142017f2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Training Loop ---\n",
        "\n",
        "def train_model(model, criterion, optimizer, num_epochs=NUM_EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0\n",
        "\n",
        "        # Iterate over data\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1) # Get predicted class\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_acc+= torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = running_acc.double() / len(train_loader.dataset)\n",
        "\n",
        "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        # VALIDATION Phase (Optional, but highly recommended)\n",
        "        model.eval() # Set model to evaluation mode\n",
        "        val_running_loss = 0.0\n",
        "        val_running_acc = 0\n",
        "\n",
        "        with torch.no_grad(): # Disable gradient calculations\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_running_loss += loss.item() * inputs.size(0)\n",
        "                val_running_acc += torch.sum(preds == labels.data)\n",
        "\n",
        "        val_loss = val_running_loss / len(val_loader.dataset)\n",
        "        val_acc = val_running_acc.double() / len(val_loader.dataset)\n",
        "\n",
        "        print(f'Val Loss: {val_loss:.4f} Val Acc: {val_acc:.4f}')\n",
        "\n",
        "    time_elapsed = time.time() - start_time\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "\n",
        "# Execute the training function\n",
        "train_model(model, criterion, optimizer)"
      ],
      "metadata": {
        "id": "VjRA67Y4gPyP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae640cab-78a9-4c42-8a91-b57eb3c6a991"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "06W5lmyol0Zv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}